{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6y1Tug4rVNW"
   },
   "source": [
    "Quantized Transfer Learning for Computer Vision Tutorial\n",
    "========================================================\n",
    "\n",
    "**Author**: [Zafar Takhirov](https://github.com/z-a-f)\n",
    "\n",
    "**Reviewed by**: [Raghuraman Krishnamoorthi](https://github.com/raghuramank100)\n",
    "\n",
    "**Edited by**: [Jessica Lin](https://github.com/jlin27)\n",
    "\n",
    "This tutorial builds on the original [PyTorch Transfer Learning](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)\n",
    "tutorial, written by [Sasank Chilamkurthy](https://chsasank.github.io/).\n",
    "Transfer learning refers to techniques that make use of a pretrained model for\n",
    "application on a different data-set.\n",
    "There are two main ways the transfer learning is used:\n",
    "1. **ConvNet as a fixed feature extractor**: Here, you [\"freeze\"](https://arxiv.org/abs/1706.04983)\n",
    "   the weights of all the parameters in the network except that of the final\n",
    "   several layers (aka “the head”, usually fully connected layers).\n",
    "   These last layers are replaced with new ones initialized with random\n",
    "   weights and only these layers are trained.\n",
    "2. **Finetuning the ConvNet**: Instead of random initializaion, the model is\n",
    "   initialized using a pretrained network, after which the training proceeds as\n",
    "   usual but with a different dataset.\n",
    "   Usually the head (or part of it) is also replaced in the network in\n",
    "   case there is a different number of outputs.\n",
    "   It is common in this method to set the learning rate to a smaller number.\n",
    "   This is done because the network is already trained, and only minor changes\n",
    "   are required to \"finetune\" it to a new dataset.\n",
    "   \n",
    "You can also combine the above two methods:\n",
    "First you can freeze the feature extractor, and train the head. After\n",
    "that, you can unfreeze the feature extractor (or part of it), set the\n",
    "learning rate to something smaller, and continue training.\n",
    "\n",
    "In this part you will use the first method – extracting the features\n",
    "using a quantized model.\n",
    "\n",
    "Part 0. Prerequisites\n",
    "---------------------\n",
    "Before diving into the transfer learning, let us review the \"prerequisites\",\n",
    "such as installations and data loading/visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dGvCG4gQrVNT"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AOu-B8urVNZ"
   },
   "source": [
    "### Installing the Nightly Build\n",
    "\n",
    "Because you will be using the experimental parts of the PyTorch, it is\n",
    "recommended to install the latest version of ``torch`` and\n",
    "``torchvision``. You can find the most recent instructions on local\n",
    "installation [here](https://pytorch.org/get-started/locally/).\n",
    "For example, to install without GPU support:\n",
    "\n",
    "```shell\n",
    "   pip install numpy\n",
    "   pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\n",
    "   # For CUDA support use https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RNyQYNhnrW21",
    "outputId": "3c2ff4d2-b8fb-4122-81b9-fc77b1f70532"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling torch-1.8.1+cu101:\n",
      "  Would remove:\n",
      "    /usr/local/bin/convert-caffe2-to-onnx\n",
      "    /usr/local/bin/convert-onnx-to-caffe2\n",
      "    /usr/local/lib/python3.7/dist-packages/caffe2/*\n",
      "    /usr/local/lib/python3.7/dist-packages/torch-1.8.1+cu101.dist-info/*\n",
      "    /usr/local/lib/python3.7/dist-packages/torch/*\n",
      "Proceed (y/n)? "
     ]
    }
   ],
   "source": [
    "!yes y | pip uninstall torch torchvision\n",
    "!yes y | pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu101/torch_nightly.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M04oI5xmrVNZ"
   },
   "source": [
    "### Load Data\n",
    "\n",
    "**Note:** This section is identical to the original transfer learning tutorial.\n",
    "We will use ``torchvision`` and ``torch.utils.data`` packages to load\n",
    "the data.\n",
    "\n",
    "The problem you are going to solve today is classifying **ants** and\n",
    "**bees** from images. The dataset contains about 120 training images\n",
    "each for ants and bees. There are 75 validation images for each class.\n",
    "This is considered a very small dataset to generalize on. However, since\n",
    "we are using transfer learning, we should be able to generalize\n",
    "reasonably well.\n",
    "\n",
    "*This dataset is a very small subset of imagenet.*\n",
    "\n",
    "**Note:** Download the data from [here](https://download.pytorch.org/tutorial/hymenoptera_data.zip)\n",
    "  and extract it to the ``data`` directory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UbbMxHz4Tn-T"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "DATA_URL = 'https://download.pytorch.org/tutorial/hymenoptera_data.zip'\n",
    "DATA_PATH = os.path.join('.', 'data')\n",
    "FILE_NAME = os.path.join(DATA_PATH, 'hymenoptera_data.zip')\n",
    "\n",
    "if not os.path.isfile(FILE_NAME):\n",
    "  print(\"Downloading the data...\")\n",
    "  os.makedirs('data', exist_ok=True)\n",
    "  with requests.get(DATA_URL) as req:\n",
    "    with open(FILE_NAME, 'wb') as f:\n",
    "      f.write(req.content)\n",
    "  if 200 <= req.status_code < 300:\n",
    "    print(\"Download complete!\")\n",
    "  else:\n",
    "    print(\"Download failed!\")\n",
    "else:\n",
    "  print(FILE_NAME, \"already exists, skipping download...\")\n",
    "\n",
    "with zipfile.ZipFile(FILE_NAME, 'r') as zip_ref:\n",
    "  print(\"Unzipping...\")\n",
    "  zip_ref.extractall('data')\n",
    "\n",
    "DATA_PATH = os.path.join(DATA_PATH, 'hymenoptera_data')\n",
    "print(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuYUNKLxshUX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Data augmentation and normalization for training\n",
    "# Just normalization for validation\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "data_dir = './data/hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
    "                                              shuffle=True, num_workers=8)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMwJRlaIrVNc"
   },
   "source": [
    "### Visualize a few images\n",
    "\n",
    "\n",
    "Let's visualize a few training images so as to understand the data\n",
    "augmentations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WqLJjE5rrVNd"
   },
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "def imshow(inp, title=None, ax=None, figsize=(5, 5)):\n",
    "  \"\"\"Imshow for Tensor.\"\"\"\n",
    "  inp = inp.numpy().transpose((1, 2, 0))\n",
    "  mean = np.array([0.485, 0.456, 0.406])\n",
    "  std = np.array([0.229, 0.224, 0.225])\n",
    "  inp = std * inp + mean\n",
    "  inp = np.clip(inp, 0, 1)\n",
    "  if ax is None:\n",
    "    fig, ax = plt.subplots(1, figsize=figsize)\n",
    "  ax.imshow(inp)\n",
    "  ax.set_xticks([])\n",
    "  ax.set_yticks([])\n",
    "  if title is not None:\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs, nrow=4)\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "imshow(out, title=[class_names[x] for x in classes], ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkofpspKrVNf"
   },
   "source": [
    "### Support Function for Model Training\n",
    "\n",
    "Below is a generic function for model training.\n",
    "This function also\n",
    "\n",
    "- Schedules the learning rate\n",
    "- Saves the best model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXPlVBc4rVNg"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, device='cpu'):\n",
    "      \"\"\"\n",
    "      Support function for model training.\n",
    "\n",
    "      Args:\n",
    "        model: Model to be trained\n",
    "        criterion: Optimization criterion (loss)\n",
    "        optimizer: Optimizer to use for training\n",
    "        scheduler: Instance of ``torch.optim.lr_scheduler``\n",
    "        num_epochs: Number of epochs\n",
    "        device: Device to run the training on. Must be 'cpu' or 'cuda'\n",
    "      \"\"\"\n",
    "      since = time.time()\n",
    "\n",
    "      best_model_wts = copy.deepcopy(model.state_dict())\n",
    "      best_acc = 0.0\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "          if phase == 'train':\n",
    "            model.train()  # Set model to training mode\n",
    "          else:\n",
    "            model.eval()   # Set model to evaluate mode\n",
    "\n",
    "          running_loss = 0.0\n",
    "          running_corrects = 0\n",
    "\n",
    "          # Iterate over data.\n",
    "          for inputs, labels in dataloaders[phase]:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            # track history if only in train\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "              outputs = model(inputs)\n",
    "              _, preds = torch.max(outputs, 1)\n",
    "              loss = criterion(outputs, labels)\n",
    "\n",
    "              # backward + optimize only if in training phase\n",
    "              if phase == 'train':\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "          if phase == 'train':\n",
    "            scheduler.step()\n",
    "\n",
    "          epoch_loss = running_loss / dataset_sizes[phase]\n",
    "          epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "          print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "            phase, epoch_loss, epoch_acc))\n",
    "\n",
    "          # deep copy the model\n",
    "          if phase == 'val' and epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "      time_elapsed = time.time() - since\n",
    "      print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "      print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "      # load best model weights\n",
    "      model.load_state_dict(best_model_wts)\n",
    "      return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iK7budzrVNi"
   },
   "source": [
    "### Support Function for Visualizing the Model Predictions\n",
    "\n",
    "Generic function to display predictions for a few images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFJ7b7dmrVNi"
   },
   "outputs": [],
   "source": [
    " def visualize_model(model, rows=3, cols=3):\n",
    "      was_training = model.training\n",
    "      model.eval()\n",
    "      current_row = current_col = 0\n",
    "      fig, ax = plt.subplots(rows, cols, figsize=(cols*2, rows*2))\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for idx, (imgs, lbls) in enumerate(dataloaders['val']):\n",
    "          imgs = imgs.cpu()\n",
    "          lbls = lbls.cpu()\n",
    "\n",
    "          outputs = model(imgs)\n",
    "          _, preds = torch.max(outputs, 1)\n",
    "\n",
    "          for jdx in range(imgs.size()[0]):\n",
    "            imshow(imgs.data[jdx], ax=ax[current_row, current_col])\n",
    "            ax[current_row, current_col].axis('off')\n",
    "            ax[current_row, current_col].set_title('predicted: {}'.format(class_names[preds[jdx]]))\n",
    "\n",
    "            current_col += 1\n",
    "            if current_col >= cols:\n",
    "              current_row += 1\n",
    "              current_col = 0\n",
    "            if current_row >= rows:\n",
    "              model.train(mode=was_training)\n",
    "              return\n",
    "        model.train(mode=was_training)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqNqqR3LrVNk"
   },
   "source": [
    "## Part 1. Training a Custom Classifier based on a Quantized Feature Extractor \n",
    "\n",
    "In this section you will use a “frozen” quantized feature extractor, and\n",
    "train a custom classifier head on top of it. Unlike floating point\n",
    "models, you don’t need to set requires_grad=False for the quantized\n",
    "model, as it has no trainable parameters. Please, refer to the[documentation](https://pytorch.org/docs/stable/quantization.html) for\n",
    "more details.\n",
    "\n",
    "Load a pretrained model: for this exercise you will be using [ResNet-18](https://pytorch.org/hub/pytorch_vision_resnet/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F7Mws9H9rVNl"
   },
   "outputs": [],
   "source": [
    "import torchvision.models.quantization as models\n",
    "\n",
    "# We will need the number of filters in the `fc` for future use.\n",
    "# Here the size of each output sample is set to 2.\n",
    "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
    "model_fe = models.resnet18(pretrained=True, progress=True, quantize=True)\n",
    "num_ftrs = model_fe.fc.in_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcNwufxZrVNm"
   },
   "source": [
    "At this point you need to modify the pretrained model. The model\n",
    "has the quantize/dequantize blocks in the beginning and the end. However,\n",
    "because you will only use the feature extractor, the dequantizatioin layer has\n",
    "to move right before the linear layer (the head). The easiest way to do that\n",
    "is to wrap the model in the ``nn.Sequential`` module.\n",
    "\n",
    "The first step is to isolate the feature extractor in the ResNet\n",
    "model. Although in this example you are tasked to use all layers except\n",
    "``fc`` as the feature extractor, in reality, you can take as many parts\n",
    "as you need. This would be useful in case you would like to replace some\n",
    "of the convolutional layers as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhhPurYJrVNn"
   },
   "source": [
    "**Note:** When separating the feature extractor from the rest of a quantized\n",
    "   model, you have to manually place the quantizer/dequantized in the\n",
    "   beginning and the end of the parts you want to keep quantized.\n",
    "\n",
    "The function below creates a model with a custom head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SM_KR35FrVNn",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def create_combined_model(model_fe):\n",
    "  # Step 1. Isolate the feature extractor.\n",
    "  model_fe_features = nn.Sequential(\n",
    "    model_fe.quant,  # Quantize the input\n",
    "    model_fe.conv1,\n",
    "    model_fe.bn1,\n",
    "    model_fe.relu,\n",
    "    model_fe.maxpool,\n",
    "    model_fe.layer1,\n",
    "    model_fe.layer2,\n",
    "    model_fe.layer3,\n",
    "    model_fe.layer4,\n",
    "    model_fe.avgpool,\n",
    "    model_fe.dequant,  # Dequantize the output\n",
    "  )\n",
    "\n",
    "  # Step 2. Create a new \"head\"\n",
    "  new_head = nn.Sequential(\n",
    "    nn.Dropout(p=0.5),\n",
    "    nn.Linear(num_ftrs, 2),\n",
    "  )\n",
    "\n",
    "  # Step 3. Combine, and don't forget the quant stubs.\n",
    "  new_model = nn.Sequential(\n",
    "    model_fe_features,\n",
    "    nn.Flatten(1),\n",
    "    new_head,\n",
    "  )\n",
    "  return new_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyIn2_MarVNp"
   },
   "source": [
    "**Warning:** Currently the quantized models can only be run on CPU.\n",
    "  However, it is possible to send the non-quantized parts of the model to a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i_01GgyErVNq"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "new_model = create_combined_model(model_fe)\n",
    "new_model = new_model.to('cpu')\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Note that we are only training the head.\n",
    "optimizer_ft = optim.SGD(new_model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZarnvYk_rVNr"
   },
   "source": [
    "### Train and evaluate\n",
    "\n",
    "This step takes around 15-25 min on CPU. Because the quantized model can\n",
    "only run on the CPU, you cannot run the training on GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kMr1jz7MrVNs",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "new_model = train_model(new_model, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                        num_epochs=25, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3SD1z15crVNu"
   },
   "outputs": [],
   "source": [
    "visualize_model(new_model)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHK0ccqp6vUY"
   },
   "source": [
    "## Part 2. Finetuning the Quantizable Model\n",
    "\n",
    "\n",
    "In this part, we fine tune the feature extractor used for transfer\n",
    "learning, and quantize the feature extractor. Note that in both part 1\n",
    "and 2, the feature extractor is quantized. The difference is that in\n",
    "part 1, we use a pretrained quantized model. In this part, we create a\n",
    "quantized feature extractor after fine tuning on the data-set of\n",
    "interest, so this is a way to get better accuracy with transfer learning\n",
    "while having the benefits of quantization. Note that in our specific\n",
    "example, the training set is really small (120 images) so the benefits\n",
    "of fine tuning the entire model is not apparent. However, the procedure\n",
    "shown here will improve accuracy for transfer learning with larger\n",
    "datasets.\n",
    "\n",
    "The pretrained feature extractor must be quantizable.\n",
    "To make sure it is quantizable, perform the following steps:\n",
    "\n",
    " 1. Fuse ``(Conv, BN, ReLU)``, ``(Conv, BN)``, and ``(Conv, ReLU)`` using\n",
    "    ``torch.quantization.fuse_modules``.\n",
    " 2. Connect the feature extractor with a custom head.\n",
    "    This requires dequantizing the output of the feature extractor.\n",
    " 3. Insert fake-quantization modules at appropriate locations\n",
    "    in the feature extractor to mimic quantization during training.\n",
    "\n",
    "For step (1), we use models from ``torchvision/models/quantization``, which\n",
    "have a member method ``fuse_model``. This function fuses all the ``conv``,\n",
    "``bn``, and ``relu`` modules. For custom models, this would require calling\n",
    "the ``torch.quantization.fuse_modules`` API with the list of modules to fuse\n",
    "manually.\n",
    "\n",
    "Step (2) is performed by the ``create_combined_model`` function\n",
    "used in the previous section.\n",
    "\n",
    "Step (3) is achieved by using ``torch.quantization.prepare_qat``, which\n",
    "inserts fake-quantization modules.\n",
    "\n",
    "\n",
    "As step (4), you can start \"finetuning\" the model, and after that convert\n",
    "it to a fully quantized version (Step 5).\n",
    "\n",
    "To convert the fine tuned model into a quantized model you can call the\n",
    "``torch.quantization.convert`` function (in our case only\n",
    "the feature extractor is quantized).\n",
    "\n",
    "**Note:** Because of the random initialization your results might differ from\n",
    "   the results shown in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBZG35ZQ59rq"
   },
   "outputs": [],
   "source": [
    "# notice `quantize=False`\n",
    "model = models.resnet18(pretrained=True, progress=True, quantize=False)\n",
    "num_ftrs = model.fc.in_features\n",
    "\n",
    "# Step 1\n",
    "model.train()\n",
    "model.fuse_model()\n",
    "# Step 2\n",
    "model_ft = create_combined_model(model)\n",
    "model_ft[0].qconfig = torch.quantization.default_qat_qconfig  # Use default QAT configuration\n",
    "# Step 3\n",
    "model_ft = torch.quantization.prepare_qat(model_ft, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wpc9W-0HFfQ"
   },
   "source": [
    "### Finetuning the model\n",
    "\n",
    "In the current tutorial the whole model is fine tuned. In\n",
    "general, this will lead to higher accuracy. However, due to the small\n",
    "training set used here, we end up overfitting to the training set.\n",
    "\n",
    "\n",
    "Step 4. Fine tune the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QGMVVrcsHJMc"
   },
   "outputs": [],
   "source": [
    "for param in model_ft.parameters():\n",
    "  param.requires_grad = True\n",
    "\n",
    "model_ft.to(device)  # We can fine-tune on GPU if available\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Note that we are training everything, so the learning rate is lower\n",
    "# Notice the smaller learning rate\n",
    "optimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9, weight_decay=0.1)\n",
    "\n",
    "# Decay LR by a factor of 0.3 every several epochs\n",
    "exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer_ft, step_size=5, gamma=0.3)\n",
    "\n",
    "model_ft_tuned = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
    "                              num_epochs=25, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4A9WvbUlHIzT"
   },
   "source": [
    "Step 5. Convert to quantized model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jWm3bw8zHO1T"
   },
   "outputs": [],
   "source": [
    "from torch.quantization import convert\n",
    "model_ft_tuned.cpu()\n",
    "\n",
    "model_quantized_and_trained = convert(model_ft_tuned, inplace=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAVbFinzgXYM"
   },
   "source": [
    "Lets see how the quantized model performs on a few images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nD9AwylKHR7k"
   },
   "outputs": [],
   "source": [
    "visualize_model(model_quantized_and_trained)\n",
    "\n",
    "plt.ioff()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1W-h7bzrZ2_n"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": " Quantized Transfer Learning",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
