{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Imports and Initializations</h1>\n",
    "<ul>\n",
    "<li><b>dataset_id</b> is the id of the google bigQuery database</li>\n",
    "<li><b>table</b> is the id of the google biqQuery database table</li>\n",
    "<li><b>bicket_name</b> is the name of the google bucket</li>\n",
    "<li><b>root</b> is the name of the google bucket location, \"data\" is the directory with entire dataset, where as \"data_sample\" only has 80 data points for each class.</li>\n",
    "</ul>\n",
    "<p style=\"color:blue\">Its never a good idea to save blobs in any database, rather we prefer to save only the reference of blobs in database, same approach is taken here, The actual data will stored in google bucket but will be handle using biqQuery database</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "import threading\n",
    "\n",
    "from datetime import datetime\n",
    "from ipywidgets import IntProgress\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "\n",
    "dataset_id = 'dspd_aftabkhalil_dataset'\n",
    "table_id = 'sounds_sample'\n",
    "\n",
    "bucket_name = \"dspd_aftabkhalil_bucket\"\n",
    "\n",
    "#Actual data was uploded fron the './data' folder, but to reduce submission size now only data_sample is available\n",
    "#to check the working of code\n",
    "root = './data_sample'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create biqQuery dataset</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exixts myfirstproject-305412.dspd_aftabkhalil_dataset\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(dataset_id):\n",
    "    bigquery_client = bigquery.Client()\n",
    "    \n",
    "    datasets = bigquery_client.list_datasets()\n",
    "    dataset = next((d for d in datasets if d.dataset_id == dataset_id), None)\n",
    "\n",
    "    if(dataset != None):\n",
    "        print(f'Dataset already exixts {bigquery_client.project}.{dataset.dataset_id}')\n",
    "        return dataset\n",
    "    else:\n",
    "        dataset_full_id = f'{bigquery_client.project}.{dataset_id}'\n",
    "        dataset_object = bigquery.Dataset(dataset_full_id)\n",
    "        dataset = bigquery_client.create_dataset(dataset_object, timeout = 30)\n",
    "        print(f'Created dataset {bigquery_client.project}.{dataset.dataset_id}')\n",
    "    return dataset\n",
    "\n",
    "_ = create_dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create table</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table myfirstproject-305412.dspd_aftabkhalil_dataset.sounds_sample\n"
     ]
    }
   ],
   "source": [
    "def create_table(dataset_id, table_id, schema):\n",
    "    bigquery_client = bigquery.Client()\n",
    "    dataset_full_id = f'{bigquery_client.project}.{dataset_id}'\n",
    "    \n",
    "    tables = bigquery_client.list_tables(dataset_full_id)\n",
    "    table = next((t for t in tables if t.table_id == table_id), None)\n",
    "    if(table != None):\n",
    "        print(f'Table already exixts {dataset_full_id}.{table.table_id}')\n",
    "        return table\n",
    "    else:\n",
    "        table_full_id = f'{dataset_full_id}.{table_id}'\n",
    "        table_obj = bigquery.Table(table_full_id, schema = schema)\n",
    "        table = bigquery_client.create_table(table_obj)\n",
    "        print(f'Created table {dataset_full_id}.{table.table_id}')\n",
    "        return table\n",
    "              \n",
    "table_schema = [\n",
    "    bigquery.SchemaField(\"type\", \"STRING\", mode = \"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"name\", \"STRING\", mode = \"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"location\", \"STRING\", mode = \"REQUIRED\"),\n",
    "]\n",
    "\n",
    "table = create_table(dataset_id, table_id, table_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Function to insert data in biqQuery table</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigquery_client = bigquery.Client()\n",
    "\n",
    "def insert_row_into_table(dataset_id, table_id, resource_type, name, location):\n",
    "    rows = [{u\"type\": resource_type, u\"name\": name, u\"location\": location}]\n",
    "    table_full_id = f'{bigquery_client.project}.{dataset_id}.{table_id}'\n",
    "    bigquery_client.insert_rows_json(table_full_id, rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Create google bucket</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket already exixts dspd_aftabkhalil_bucket in US with storage class STANDARD\n"
     ]
    }
   ],
   "source": [
    "def create_or_get_bucket(bucket_name):\n",
    "    \n",
    "    #Create storage client\n",
    "    storage_client = storage.Client()\n",
    "    \n",
    "    #Get already existsing buckets\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    \n",
    "    #Check if required bucket already exists\n",
    "    bucket = next((b for b in buckets if b.name == bucket_name), None)\n",
    "    \n",
    "    #If bucket already exists retuen it\n",
    "    if(bucket != None):\n",
    "        print(f'Bucket already exixts {bucket.name} in {bucket.location} with storage class {bucket.storage_class}')\n",
    "        return bucket\n",
    "    #Else create and return bucket\n",
    "    else:\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        new_bucket = storage_client.create_bucket(bucket, location=\"us\")\n",
    "        print(f'Created bucket {new_bucket.name} in {new_bucket.location} with storage class {new_bucket.storage_class}')\n",
    "        return new_bucket\n",
    "\n",
    "_ = create_or_get_bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Function to upload blob to google bucket and insert entry in bigQuery table</h1>\n",
    "<p>Note that if a file with same name already exists in bucket, it will be uploaded iff force_upload is set to True</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "def upload_blob(root, resource_type, resource_name, force_upload = False):\n",
    "    resource_full_path = f'{root}/{resource_type}/{resource_name}'\n",
    "    blob = bucket.blob(resource_full_path)\n",
    "    if(force_upload or not blob.exists()):\n",
    "        blob.upload_from_filename(resource_full_path)\n",
    "        insert_row_into_table(dataset_id, table_id, resource_type, resource_name, resource_full_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lets Upload!</h1>\n",
    "<p>We will upload the data via multithreaded uproach<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are a total of 2400 files in dataset.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b36df7cef914af993358fad68e0eec7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, max=2400)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload complete\n"
     ]
    }
   ],
   "source": [
    "threads = list()\n",
    "def upload_dataset(root_folder, retry = True):\n",
    "    max_count = len(list(root_folder.glob('*/*.wav')))\n",
    "    \n",
    "    print(f'There are a total of {max_count} files in dataset.')    \n",
    "    uploadBar = IntProgress(min = 0, max = max_count)\n",
    "    display(uploadBar)\n",
    "    \n",
    "    for dir in os.listdir(root_folder):\n",
    "        for file in os.listdir(os.path.join(root_folder, dir)):\n",
    "            t = threading.Thread(target = upload_blob, args = (root_folder, dir, file))\n",
    "            t.start()\n",
    "            threads.append(t)   \n",
    "            uploadBar.value += 1\n",
    "            \n",
    "            #Wait between threads, or api call limit will be reached\n",
    "            if retry:\n",
    "                time.sleep(0.01)\n",
    "            \n",
    "    for t in threads:\n",
    "        t.join()\n",
    "        \n",
    "data_dir = pathlib.Path(root)          \n",
    "upload_dataset(data_dir)\n",
    "print('Upload complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Please wait until the above cell prints Upload complete.. üêç</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dspd",
   "language": "python",
   "name": "dspd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
